# Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import json
import os


class GlobalManager:
    json_file = os.path.dirname(__file__) + "/api_mapping.json"
    with open(json_file, "r") as file:
        API_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_wildcard_mapping.json"
    with open(json_file, "r") as file:
        API_WILDCARD_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/attribute_mapping.json"
    with open(json_file, "r") as file:
        ATTRIBUTE_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_alias_mapping.json"
    with open(json_file, "r") as file:
        ALIAS_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_alias_mapping.json"
    with open(json_file, "r") as file:
        ALIAS_MAPPING = json.load(file)

    # used to replace import (means replace api by all)
    IMPORT_PACKAGE_MAPPING = {
        "audiotools": "paddlespeech.audiotools",
    }
    # used to replace api one by one
    # Abbreviation after annotation as the prefix for corresponding matcher
    TORCH_PACKAGE_MAPPING = {
        "torch": "paddle",
        "mmseg": "paddle",
        "mmcv": "paddle",
        "mmdet": "paddle",
        "mmdet3d": "paddle",
        "mmengine": "paddle",
        "detectron": "paddle",
        "timm": "paddle",
        "torchvision": "paddle",
        "torchaudio": "paddlespeech",
        "kornia": "paddle",
        "fasttext": "paddle",
        "pytorch_lightning": "paddle",
        "lightning": "paddle",
        "jieba": "paddle",
        "NLTK": "paddle",
        "scikit-learn": "paddle",
        "fairscale": "paddle",  # FS
        "transformers": "paddleformers",  # TRFM
        "datasets": "paddle",
        "accelerate": "paddle",
        "diffusers": "paddle",
        "torch_xla": "paddle",
        "flash_attn": "paddle",  # FA
    }
    MAY_TORCH_PACKAGE_LIST = [
        "setuptools",
        "os",
        "einops",
    ]

    # 无需转换的Pytorch API名单
    NO_NEED_CONVERT_LIST = [
        # zhimin
        "torch.Tensor.bfloat16",
        "torch.Tensor.bool",
        "torch.Tensor.byte",
        "torch.Tensor.char",
        "torch.Tensor.double",
        "torch.Tensor.float",
        "torch.Tensor.half",
        "torch.Tensor.int",
        "torch.Tensor.long",
        "torch.Tensor.short",
        "torch.Tensor.cfloat",
        "torch.Tensor.cdouble",
        "torch.nn.init.calculate_gain",
        "torch.nn.init.constant_",
        "torch.nn.init.dirac_",
        "torch.nn.init.eye_",
        "torch.nn.init.kaiming_normal_",
        "torch.nn.init.kaiming_uniform_",
        "torch.nn.init.normal_",
        "torch.nn.init.ones_",
        "torch.nn.init.orthogonal_",
        "torch.nn.init.trunc_normal_",
        "torch.nn.init.uniform_",
        "torch.nn.init.xavier_normal_",
        "torch.nn.init.xavier_uniform_",
        "torch.nn.init.zeros_",
        "torch.nn.Conv1d",
        "torch.nn.Conv2d",
        "torch.nn.Conv3d",
        "torch.nn.Embedding",

        # zhouxin
        "torch.complex",
        "torch.polar",
        "torch.cat",
        "torch.stack",
        "torch.swapaxes",
        "torch.swapdims",
        "torch.where",
        "torch.clamp",
        "torch.clip",
        "torch.cos",
        "torch.floor",
        "torch.log",
        "torch.mul",
        "torch.multiply",
        "torch.pow",
        "torch.rsqrt",
        "torch.sign",
        "torch.sin",
        "torch.eq",
        "torch.gt",
        "torch.view_as_real",
        "torch.view_as_complex",
        "torch.ger",
        "torch.Tensor.mul",
        "torch.Tensor.mul_",
        "torch.Tensor.swapaxes",
        "torch.Tensor.swapdims",
        "torch.autograd.Function",
        "torch.take_along_dim",
        "torch.Tensor.take_along_dim",
        "torch.special.logsumexp",
        "torch.argwhere",
        "torch.concatenate",
        "torch.is_autocast_enabled",
        "torch.get_autocast_gpu_dtype",
        "torch.cumsum",
        "torch.diff",


        # honggeng
        "torch.nn.functional.dropout1d",
        "torch.nn.parameter.Parameter",
        "torch.add",
        "torch.div",
        "torch.divide",
        "torch.true_divide",
        "torch.Tensor.add",
        "torch.Tensor.add_",
        "torch.Tensor.div",
        "torch.Tensor.div_",
        "torch.Tensor.divide",
        "torch.Tensor.divide_",
        "torch.Tensor.true_divide",
        "torch.Size",
        "torch.Tensor.is_cuda",
        "torch.sub",
        "torch.Tensor.sub",
        "torch.Tensor.sub_",
        "torch.Tensor.random_",


        # sensen
        "torch.range",
        "torch.arange",
        "torch.randn",
        "torch.zeros",
        "torch.ones",
        "torch.full",
        "torch.empty",
        "torch.zeros_like",
        "torch.ones_like",
        "torch.full_like",
        "torch.empty_like",
        "torch.Tensor.new_zeros",
        "torch.Tensor.new_ones",
        "torch.Tensor.new_full",
        "torch.Tensor.new_empty",
        "torch.eye",
        "torch.cuda.cudart",
        "torch.cuda.check_error",
        "torch.cuda.mem_get_info",
        "torch.cuda.is_initialized",

        # hongyu
        "torch.permute",
        "torch.Tensor.permute",
        "torch.repeat_interleave",
        "torch.Tensor.repeat_interleave",
        "torch.Tensor.repeat",
        "torch.maximum",
        "torch.minimum",
        "torch.topk",
        "torch.sqrt",
        "torch.amin",
        "torch.amax",
        "torch.Tensor.stride",
        "torch.Tensor.get_device",
        "torch.random.initial_seed",

        # linjun
        "torch.as_tensor", 
        "torch.tensor",
        "torch.Tensor.copy_",
        "torch.Tensor.norm",
        "torch.Tensor",
        "torch.FloatTensor", 
        "torch.DoubleTensor",
        "torch.HalfTensor",
        "torch.BFloat16Tensor",
        "torch.ByteTensor",
        "torch.CharTensor",
        "torch.ShortTensor",
        "torch.IntTensor",
        "torch.LongTensor",
        "torch.BoolTensor",
        "torch.norm",
        "torch.linalg.norm",
        "torch.Tensor.size",
        "torch.Tensor.to",
        "torch.nn.Module.to",
        "torch.linalg.vector_norm",
        "torch.functional.split",
        "torch.functional.unique_consecutive",
        "torch.functional.atleast_1d",
        "torch.functional.atleast_2d",
        "torch.functional.atleast_3d",
        "torch.functional.broadcast_shapes",
        "torch.functional.einsum",
        "torch.functional.norm",

        # siyu
        "torch.multinomial",
        "torch.var",
        "torch.rand_like",
        "torch.mean",
        "torch.Tensor.mean",


        # shijie
        "torch.msort",
        "torch.Tensor.msort",
        "torch.Tensor.ravel",
        "torch.ravel",
        "torch.Tensor.scatter_add",
        "torch.scatter_add",
        "torch.Tensor.scatter_add_",
        "torch.Tensor.tril",
        "torch.tril",
        "torch.Tensor.triu",
        "torch.triu",
        "torch.bmm",
        "torch.Tensor.bmm",
        "torch.nn.GELU",
        "torch.broadcast_shapes",
        "torch.Tensor.scatter_reduce",
        "torch.scatter_reduce",
        "torch.nn.functional.silu",

        # yuyan
        "torch.Tensor.softmax",
        "torch.special.softmax",
        "torch.softmax",
        "torch.Tensor.clamp",
        "torch.Tensor.itemsize",

        # huoda
        "torch.get_default_dtype",
        "torch.einsum",
        "torch.nn.Identity",
        "torch.Tensor.ndim",
        "torch.Tensor.T",
        "torch.Tensor.abs",
        "torch.Tensor.cos",
        "torch.Tensor.detach",
        "torch.Tensor.dim",
        "torch.Tensor.fill_",
        "torch.Tensor.isnan",
        "torch.Tensor.item",
        "torch.Tensor.log",
        "torch.Tensor.masked_scatter",
        "torch.Tensor.masked_fill_",
        "torch.Tensor.masked_fill",
        "torch.Tensor.nonzero",
        "torch.Tensor.normal_",
        "torch.Tensor.sigmoid",
        "torch.Tensor.sin",
        "torch.Tensor.square",
        "torch.Tensor.tolist",
        "torch.Tensor.zero_",
        "torch.distributed.get_rank",
        "torch.distributed.get_world_size",
        "torch.special.softmax",
        "torch.Tensor.shape",
        "torch.float32",
        "torch.long",
        "torch.int32",
        "torch.bfloat16",
        "torch.int64",
        "torch.bool",
        "torch.uint8",
        "torch.Tensor.abs_",
        "torch.Tensor.acos",
        "torch.Tensor.acos_",
        "torch.Tensor.acosh",
        "torch.Tensor.acosh_",
        "torch.Tensor.angle",
        "torch.Tensor.apply_",
        "torch.Tensor.asin",
        "torch.Tensor.asin_",
        "torch.Tensor.asinh",
        "torch.Tensor.asinh_",
        "torch.Tensor.atan",
        "torch.Tensor.atan_",
        "torch.Tensor.atanh",
        "torch.Tensor.atanh_",
        "torch.Tensor.bincount",
        "torch.Tensor.bitwise_not",
        "torch.Tensor.bitwise_not_",
        "torch.Tensor.ceil",
        "torch.Tensor.ceil_",
        "torch.Tensor.cholesky",
        "torch.Tensor.cholesky_inverse",
        "torch.Tensor.clip",
        "torch.Tensor.clip_",
        "torch.Tensor.coalesce",
        "torch.Tensor.conj",
        "torch.Tensor.cos_",
        "torch.Tensor.cosh",
        "torch.Tensor.cosh_",
        "torch.Tensor.cumprod",
        "torch.Tensor.cumprod_",
        "torch.Tensor.data_ptr",
        "torch.Tensor.deg2rad",
        "torch.Tensor.dense_dim",
        "torch.Tensor.detach_",
        "torch.Tensor.diag_embed",
        "torch.Tensor.diagflat",
        "torch.Tensor.digamma",
        "torch.Tensor.digamma_",
        "torch.Tensor.dtype",
        "torch.Tensor.erf",
        "torch.Tensor.erfinv",
        "torch.Tensor.erfinv_",
        "torch.Tensor.exp",
        "torch.Tensor.exp_",
        "torch.Tensor.expm1",
        "torch.Tensor.floor",
        "torch.Tensor.floor_",
        "torch.Tensor.frac",
        "torch.Tensor.frac_",
        "torch.Tensor.frexp",
        "torch.Tensor.grad",
        "torch.Tensor.i0",
        "torch.Tensor.i0_",
        "torch.Tensor.indices",
        "torch.Tensor.inverse",
        "torch.Tensor.is_complex",
        "torch.Tensor.is_floating_point",
        "torch.Tensor.is_leaf",
        "torch.Tensor.isfinite",
        "torch.Tensor.isinf",
        "torch.Tensor.isneginf",
        "torch.Tensor.isposinf",
        "torch.Tensor.isreal",
        "torch.Tensor.istft",
        "torch.Tensor.lgamma",
        "torch.Tensor.lgamma_",
        "torch.Tensor.log10",
        "torch.Tensor.log10_",
        "torch.Tensor.log1p",
        "torch.Tensor.log1p_",
        "torch.Tensor.log2",
        "torch.Tensor.log2_",
        "torch.Tensor.log_",
        "torch.Tensor.logit",
        "torch.Tensor.logit_",
        "torch.Tensor.lu",
        "torch.Tensor.mT",
        "torch.Tensor.masked_scatter_",
        "torch.Tensor.masked_select",
        "torch.Tensor.matrix_power",
        "torch.Tensor.mm",
        "torch.Tensor.moveaxis",
        "torch.Tensor.mv",
        "torch.Tensor.nan_to_num",
        "torch.Tensor.nan_to_num_",
        "torch.Tensor.ndimension",
        "torch.Tensor.neg",
        "torch.Tensor.neg_",
        "torch.Tensor.pin_memory",
        "torch.Tensor.polygamma",
        "torch.Tensor.polygamma_",
        "torch.Tensor.rad2deg",
        "torch.Tensor.reciprocal",
        "torch.Tensor.reciprocal_",
        "torch.Tensor.register_hook",
        "torch.Tensor.rsqrt",
        "torch.Tensor.rsqrt_",
        "torch.Tensor.sgn",
        "torch.Tensor.sigmoid_",
        "torch.Tensor.sign",
        "torch.Tensor.signbit",
        "torch.Tensor.sin_",
        "torch.Tensor.sinc",
        "torch.Tensor.sinc_",
        "torch.Tensor.sinh",
        "torch.Tensor.sinh_",
        "torch.Tensor.sparse_dim",
        "torch.Tensor.sqrt",
        "torch.Tensor.sqrt_",
        "torch.Tensor.t",
        "torch.Tensor.t_",
        "torch.Tensor.tan",
        "torch.Tensor.tan_",
        "torch.Tensor.tanh",
        "torch.Tensor.tanh_",
        "torch.Tensor.to_dense",
        "torch.Tensor.tril_",
        "torch.Tensor.triu_",
        "torch.Tensor.trunc",
        "torch.Tensor.trunc_",
        "torch.Tensor.values",
        "torch.__version__",
        "torch.__version__.split",
        "torch.diag_embed",
        "torch.distributed.ReduceOp.MAX",
        "torch.distributed.ReduceOp.MIN",
        "torch.distributed.ReduceOp.SUM",
        "torch.distributed.batch_isend_irecv",
        "torch.distributed.get_backend",
        "torch.distributed.is_available",
        "torch.distributed.is_initialized",
        "torch.e",
        "torch.enable_grad",
        "torch.inf",
        "torch.is_grad_enabled",
        "torch.nan",
        "torch.newaxis",
        "torch.nn.LogSigmoid",
        "torch.nn.Sigmoid",
        "torch.nn.Softplus",
        "torch.nn.Softsign",
        "torch.nn.Tanh",
        "torch.nn.Tanhshrink",
        "torch.nn.TransformerDecoder",
        "torch.nn.TripletMarginWithDistanceLoss",
        "torch.nn.utils.parameters_to_vector",
        "torch.nn.utils.vector_to_parameters",
        "torch.pi",
        "torch.set_default_dtype",
        "torch.t",
        "torch.utils.cpp_extension.BuildExtension",
        "torch.utils.cpp_extension.BuildExtension.with_options",
        "torch.is_grad_enabled",
        "torch.nn.Conv2d",
        "torch.nn.init.calculate_gain",
        "torch.nn.init.ones_",
        "torch.nn.init.uniform_",
        "torch.nn.init.zeros_",
        "torch.Tensor.div",
        "torch.Tensor.element_size",
        "torch.Tensor.is_floating_point",
        "torch.Tensor.neg",
        "torch.Tensor.pin_memory",
        "torch.Tensor.view_as",
        "torch.distributed.is_available",
        "torch.distributed.is_initialized",
        "torch.set_default_dtype",
        "torch.dtype",
        "torch.Tensor.data_ptr",
        "torch.Tensor.__and__",
        "torch.Tensor.__array__",
        "torch.Tensor.__bool__",
        "torch.Tensor.__eq__",
        "torch.Tensor.__format__",
        "torch.Tensor.__getitem__",
        "torch.Tensor.__index__",
        "torch.Tensor.__invert__",
        "torch.Tensor.__len__",
        "torch.Tensor.__or__",
        "torch.Tensor.__rpow__",
        "torch.Tensor.__rsub__",
        "torch.Tensor.__rtruediv__",
        "torch.Tensor.__setitem__",

        # sundong
        "torch.matmul",
        "torch.linalg.matmul",
        "torch.mul",
        "torch.mul_",
        "torch.multiply",
        "torch.multiply_",
        "torch.Tensor.mul",
        "torch.Tensor.mul_",
        "torch.Tensor.multiply",
        "torch.Tensor.multiply_",
        "torch.Tensor.matmul",
        "torch.amax",
        "torch.amin",
        "torch.Tensor.amax",
        "torch.Tensor.amin",
        "torch.Tensor.log2",
        "torch.log2",
        "torch.Tensor.remainder",
        "torch.remainder",

        # zhengsheng
        "torch.broadcast_to",
        "torch.nn.functional.embedding",
        "torch.no_grad",
        "torch.ones_like",
        "torch.reshape",
        "torch.take_along_dim",
        "torch.Tensor.bitwise_or_",
        "torch.Tensor.view",
        "torch.unique_consecutive",
        "torch.eye",
        "torch.full_like",
        "torch.Tensor.cumsum",
        "torch.Tensor.expand",
        "torch.clip",
        "torch.isfinite",
        "torch.isinf",
        "torch.isnan",
        "torch.flatten",
        "torch.Tensor.flatten",
        "torch.roll",
        "torch.Tensor.sum",
        "torch.sum",
        "torch.repeat_interleave",
        "torch.Tensor.repeat_interleave",
        "torch.var",
        "torch.prod",
        "torch.ceil",
        "torch.floor_divide",
        "torch.masked_select",



        # liuyi
        "torch.finfo",
        "torch.is_complex",
        "torch.concat",
        "torch.empty_like",
        "torch.full",
        "torch.nonzero",
        "torch.Tensor.pow",
        "torch.Tensor.prod",
        "torch.Tensor.reshape",
        "torch.zeros_like",
        "torch.argsort",
        "torch.Tensor.argsort",
        "torch.Tensor.squeeze",
        "torch.chunk",
        "torch.Tensor.chunk",
        "torch.any",
        "torch.nn.functional.one_hot",
        "torch.unbind",
        "torch.Tensor.unbind",
        "torch.is_floating_point",
        "torch.is_tensor",
        "torch.isin",

        # shenwei
        "torch.Tensor.expand_as",
        "torch.logsumexp",
        "torch.Tensor.logsumexp",
        "torch.argmax",
        "torch.Tensor.argmax",
        "torch.argmin",
        "torch.Tensor.argmin",
        "torch.all",
        "torch.Tensor.all",
        "torch.Tensor.any",
        "torch.tensor_split",
        "torch.nn.functional.gelu",
        "torch.layer_norm",

        # haoyang
        "torch.logical_not",
        "torch.Tensor.logical_not",
        "torch.logical_and",
        "torch.Tensor.logical_and",
        "torch.logical_or",
        "torch.Tensor.logical_or",
        "torch.logical_xor",
        "torch.Tensor.logical_xor",
        "torch.index_select",
        "torch.Tensor.index_select",
        "torch.dot",
        "torch.Tensor.dot",
        "torch.nn.functional.conv1d",
        "torch.nn.functional.conv2d",
        "torch.nn.functional.conv3d",
        "torch.conv1d",
        "torch.conv2d",
        "torch.conv3d",


        # rongrui


        # bingxin
        
        # zhichao
        "torch.bfloat16",
        "torch.bool",
        "torch.complex128",
        "torch.complex64",
        "torch.float64",
        "torch.float16",
        "torch.float32",
        "torch.int16",
        "torch.int32",
        "torch.int64",
        "torch.int8",
        "torch.ravel",
        "torch.Tensor.narrow",
        "torch.narrow",
        "torch.Tensor.type_as",
        "torch.nn.Sequential",

        # zhouwei
        "torch.torch.int32",
        "torch.transpose",
        "torch.Tensor.transpose",
        "torch.unsqueeze",
        "torch.Tensor.unsqueeze",
        "torch.sigmoid",
        "torch.Tensor.topk",
        "torch.outer",
        "torch.nn.functional.sigmoid",
        "torch.Tensor.requires_grad",
        "torch.Tensor.data",
        "torch.Tensor.element_size",
        "torch.Tensor.view_as",
        "torch.Tensor.cpu",
        "torch.Tensor.__add__",
        "torch.Tensor.__ge__",
        "torch.Tensor.__gt__",
        "torch.Tensor.__le__",
        "torch.Tensor.__lt__",
        "torch.Tensor.__mul__",
        "torch.Tensor.__ne__",
        "torch.Tensor.__neg__",
        "torch.Tensor.__radd__",
        "torch.Tensor.__rmul__",
        "torch.Tensor.__sub__",
        "torch.Tensor.__reduce_ex__",
        "torch.from_numpy",
        "torch.greater",
        "torch.nn.functional.layer_norm",
        # "torch.nn.attention.sdpa_kernel",
        "torch.Tensor.remainder",
        "torch.nn.SiLU",
        "torch.randperm",
        "torch.Tensor.ge",
        "torch.Tensor.gt",
        "torch.Tensor.le",
        "torch.Tensor.lt",
        "torch.Tensor.ne",
        "torch.manual_seed",
        "torch.asarray",
        # "torch.save",

        # qianyue
        "torch.gather",
        "torch.Tensor.gather",
        "torch.Tensor.scatter",
        "torch.Tensor.scatter_",
        "torch.scatter",

        # xiangyu
        # "torch.cuda.current_device",
        # "torch.cuda.device_count",
        # "torch.cuda.empty_cache",
        # "torch.cuda.get_device_properties",
        # "torch.cuda.get_rng_state",
        # "torch.cuda.is_current_stream_capturing",
        # "torch.cuda.manual_seed_all",
        # "torch.cuda.memory_allocated",
        # "torch.cuda.memory_reserved",
        # "torch.cuda.set_device",
        # "torch.cuda.set_rng_state",
        # "torch.get_default_device",
        # "torch.get_device_module",
        # "torch.cuda.is_available",
        # "torch.device",
    ]

# Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import json
import os


class GlobalManager:
    json_file = os.path.dirname(__file__) + "/api_mapping.json"
    with open(json_file, "r") as file:
        API_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_wildcard_mapping.json"
    with open(json_file, "r") as file:
        API_WILDCARD_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/attribute_mapping.json"
    with open(json_file, "r") as file:
        ATTRIBUTE_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_alias_mapping.json"
    with open(json_file, "r") as file:
        ALIAS_MAPPING = json.load(file)

    json_file = os.path.dirname(__file__) + "/api_alias_mapping.json"
    with open(json_file, "r") as file:
        ALIAS_MAPPING = json.load(file)

    # used to replace import (means replace api by all)
    IMPORT_PACKAGE_MAPPING = {
        "audiotools": "paddlespeech.audiotools",
    }
    # used to replace api one by one
    # Abbreviation after annotation as the prefix for corresponding matcher
    TORCH_PACKAGE_MAPPING = {
        "torch": "paddle",
        "mmseg": "paddle",
        "mmcv": "paddle",
        "mmdet": "paddle",
        "mmdet3d": "paddle",
        "mmengine": "paddle",
        "detectron": "paddle",
        "timm": "paddle",
        "torchvision": "paddle",
        "torchaudio": "paddlespeech",
        "kornia": "paddle",
        "fasttext": "paddle",
        "pytorch_lightning": "paddle",
        "lightning": "paddle",
        "jieba": "paddle",
        "NLTK": "paddle",
        "scikit-learn": "paddle",
        "fairscale": "paddle",  # FS
        "transformers": "paddleformers",  # TRFM
        "datasets": "paddle",
        "accelerate": "paddle",
        "diffusers": "paddle",
        "torch_xla": "paddle",
        "flash_attn": "paddle",  # FA
    }
    MAY_TORCH_PACKAGE_LIST = [
        "setuptools",
        "os",
        "einops",
    ]

    # 完全对齐的Pytorch API名单
    NO_NEED_CONVERT_LIST = [
        # Manfredss
        "torch.asin",

        # zhimin
        "torch.Tensor.bfloat16",
        "torch.Tensor.bool",
        "torch.Tensor.byte",
        "torch.Tensor.char",
        "torch.Tensor.double",
        "torch.Tensor.float",
        "torch.Tensor.half",
        "torch.Tensor.int",
        "torch.Tensor.long",
        "torch.Tensor.short",
        "torch.Tensor.cfloat",
        "torch.Tensor.cdouble",
        "torch.nn.init.calculate_gain",
        "torch.nn.init.constant_",
        "torch.nn.init.dirac_",
        "torch.nn.init.eye_",
        "torch.nn.init.kaiming_normal_",
        "torch.nn.init.kaiming_uniform_",
        "torch.nn.init.normal_",
        "torch.nn.init.ones_",
        "torch.nn.init.orthogonal_",
        "torch.nn.init.trunc_normal_",
        "torch.nn.init.uniform_",
        "torch.nn.init.xavier_normal_",
        "torch.nn.init.xavier_uniform_",
        "torch.nn.init.zeros_",
        "torch.nn.Conv1d",
        "torch.nn.Conv2d",
        "torch.nn.Conv3d",
        "torch.nn.Embedding",
        
        # zhouxin
        "torch.complex",
        "torch.polar",
        "torch.cat",
        "torch.stack",
        "torch.swapaxes",
        "torch.swapdims",
        "torch.where",
        "torch.clamp",
        "torch.clip",
        "torch.cos",
        "torch.floor",
        "torch.log",
        "torch.mul",
        "torch.multiply",
        "torch.pow",
        "torch.rsqrt",
        "torch.sign",
        "torch.sin",
        "torch.eq",
        "torch.gt",
        "torch.view_as_real",
        "torch.view_as_complex",
        "torch.ger",
        "torch.Tensor.mul",
        "torch.Tensor.mul_",
        "torch.Tensor.swapaxes",
        "torch.Tensor.swapdims",
        "torch.autograd.Function",
        "torch.take_along_dim",
        "torch.Tensor.take_along_dim",
        "torch.special.logsumexp",
        "torch.argwhere",
        "torch.concatenate",
        "torch.is_autocast_enabled",
        "torch.get_autocast_gpu_dtype",
        "torch.cumsum",
        "torch.diff",
        "torch.Tensor.shape",
        "torch.nn.ConstantPad1d",
        "torch.nn.ConstantPad2d",
        "torch.nn.ConstantPad3d",
        "torch.nn.ReflectionPad1d",
        "torch.nn.ReflectionPad2d",
        "torch.nn.ReflectionPad3d",
        "torch.nn.ReplicationPad1d",
        "torch.nn.ReplicationPad2d",
        "torch.nn.ReplicationPad3d",
        "torch.nn.CircularPad1d",
        "torch.nn.CircularPad2d",
        "torch.nn.CircularPad3d",
        
        # honggeng
        "torch.nn.functional.dropout1d",
        "torch.nn.parameter.Parameter",
        "torch.nn.Parameter",
        "torch.add",
        "torch.div",
        "torch.divide",
        "torch.true_divide",
        "torch.Tensor.add",
        "torch.Tensor.add_",
        "torch.Tensor.div",
        "torch.Tensor.div_",
        "torch.Tensor.divide",
        "torch.Tensor.divide_",
        "torch.Tensor.true_divide",
        "torch.Size",
        "torch.Tensor.is_cuda",
        "torch.sub",
        "torch.Tensor.sub",
        "torch.Tensor.sub_",
        "torch.Tensor.random_",
        "torch.Tensor.split_with_sizes",
        "torch.meshgrid",
        "torch.nn.Module.add_module",
        "torch.nn.Module.apply",
        "torch.nn.Module.bfloat16",
        "torch.nn.Module.buffers",
        "torch.nn.Module.children",
        "torch.nn.Module.cpu",
        "torch.nn.Module.cuda",
        "torch.nn.Module.double",
        "torch.nn.Module.eval",
        "torch.nn.Module.float",
        "torch.nn.Module.get_buffer",
        "torch.nn.Module.get_parameter",
        "torch.nn.Module.get_submodule",
        "torch.nn.Module.half",
        "torch.nn.Module.load_state_dict",
        "torch.nn.Module.modules",
        "torch.nn.Module.named_buffers",
        "torch.nn.Module.named_children",
        "torch.nn.Module.named_modules",
        "torch.nn.Module.named_parameters",
        "torch.nn.Module.parameters",
        "torch.nn.Module.register_buffer",
        "torch.nn.Module.register_module",
        "torch.nn.Module.register_parameter",
        "torch.nn.Module.requires_grad_",
        "torch.nn.Module.state_dict",
        "torch.nn.Module.train",
        "torch.nn.Module.type",
        "torch.nn.Module.xpu",
        "torch.nn.Module.zero_grad",
        "torch.nn.Module",
        "torch.nn.ModuleDict",
        "torch.nn.ModuleList",

        # torch.nn.Module.register_full_backward_hook
        # torch.nn.Module.register_forward_hook
        # torch.nn.Module.register_forward_pre_hook
        # torch.nn.Module.register_full_backward_pre_hook
        # torch.nn.Module.to_empty
        # torch.nn.Module.to

        # torch.nn.Module.set_extra_state
        # torch.nn.Module.ipu
        # torch.nn.Module.mtia
        # torch.nn.Module.register_backward_hook
        # torch.nn.Module.register_state_dict_post_hook
        # torch.nn.Module.register_state_dict_pre_hook
        # torch.nn.Module.register_load_state_dict_pre_hook
        # torch.nn.Module.register_load_state_dict_post_hook
        # torch.nn.Module.share_memory
        # torch.nn.Module.compile

        
        # sensen
        "torch.range",
        "torch.arange",
        "torch.rand",
        "torch.rand_like",
        "torch.randn",
        "torch.zeros",
        "torch.ones",
        "torch.full",
        "torch.empty",
        "torch.zeros_like",
        "torch.ones_like",
        "torch.full_like",
        "torch.empty_like",
        "torch.Tensor.new_zeros",
        "torch.Tensor.new_ones",
        "torch.Tensor.new_full",
        "torch.Tensor.new_empty",
        "torch.eye",
        "torch.cuda.cudart",
        "torch.cuda.check_error",
        "torch.cuda.mem_get_info",
        "torch.cuda.is_initialized",
        
        # hongyu
        "torch.permute",
        "torch.Tensor.permute",
        "torch.repeat_interleave",
        "torch.Tensor.repeat_interleave",
        "torch.Tensor.repeat",
        "torch.maximum",
        "torch.minimum",
        "torch.topk",
        "torch.sqrt",
        "torch.amin",
        "torch.amax",
        "torch.Tensor.stride",
        "torch.Tensor.get_device",
        "torch.random.initial_seed",
        
        # linjun
        "torch.as_tensor",
        "torch.tensor",
        "torch.Tensor.copy_",
        "torch.Tensor.norm",
        "torch.Tensor",
        "torch.FloatTensor",
        "torch.DoubleTensor",
        "torch.HalfTensor",
        "torch.BFloat16Tensor",
        "torch.ByteTensor",
        "torch.CharTensor",
        "torch.ShortTensor",
        "torch.IntTensor",
        "torch.LongTensor",
        "torch.BoolTensor",
        "torch.norm",
        "torch.linalg.norm",
        "torch.Tensor.size",
        "torch.Tensor.to",
        "torch.nn.Module.to",
        "torch.linalg.vector_norm",
        "torch.functional.split",
        "torch.functional.unique_consecutive",
        "torch.functional.atleast_1d",
        "torch.functional.atleast_2d",
        "torch.functional.atleast_3d",
        "torch.functional.broadcast_shapes",
        "torch.functional.einsum",
        "torch.functional.norm",
        
        # siyu
        "torch.multinomial",
        "torch.var",
        "torch.rand_like",
        "torch.mean",
        "torch.Tensor.mean",
        "torch.nn.functional.dropout",
        "torch.nn.Dropout",
        "torch.linspace",
        "torch.normal",

        # shijie
        "torch.msort",
        "torch.Tensor.msort",
        "torch.Tensor.ravel",
        "torch.ravel",
        "torch.Tensor.scatter_add",
        "torch.scatter_add",
        "torch.Tensor.scatter_add_",
        "torch.Tensor.tril",
        "torch.tril",
        "torch.Tensor.triu",
        "torch.triu",
        "torch.bmm",
        "torch.Tensor.bmm",
        "torch.nn.GELU",
        "torch.broadcast_shapes",
        "torch.Tensor.scatter_reduce",
        "torch.scatter_reduce",
        "torch.nn.functional.silu",
        
        # yuyan
        "torch.Tensor.softmax",
        "torch.special.softmax",
        "torch.softmax",
        "torch.Tensor.clamp",
        "torch.Tensor.itemsize",
        
        # huoda
        "torch.get_default_dtype",
        "torch.einsum",
        "torch.nn.Identity",
        "torch.Tensor.ndim",
        "torch.Tensor.T",
        "torch.Tensor.abs",
        "torch.Tensor.cos",
        "torch.Tensor.detach",
        "torch.Tensor.dim",
        "torch.Tensor.fill_",
        "torch.Tensor.isnan",
        "torch.Tensor.item",
        "torch.Tensor.log",
        "torch.Tensor.masked_scatter",
        "torch.Tensor.masked_fill_",
        "torch.Tensor.masked_fill",
        "torch.Tensor.nonzero",
        "torch.Tensor.normal_",
        "torch.Tensor.sigmoid",
        "torch.Tensor.sin",
        "torch.Tensor.square",
        "torch.Tensor.tolist",
        "torch.Tensor.zero_",
        "torch.distributed.get_rank",
        "torch.distributed.get_world_size",
        "torch.special.softmax",
        "torch.Tensor.shape",
        "torch.float32",
        "torch.long",
        "torch.int32",
        "torch.bfloat16",
        "torch.int64",
        "torch.bool",
        "torch.uint8",
        "torch.Tensor.abs_",
        "torch.Tensor.acos",
        "torch.Tensor.acos_",
        "torch.Tensor.acosh",
        "torch.Tensor.acosh_",
        "torch.Tensor.angle",
        "torch.Tensor.apply_",
        "torch.Tensor.asin",
        "torch.Tensor.asin_",
        "torch.Tensor.asinh",
        "torch.Tensor.asinh_",
        "torch.Tensor.atan",
        "torch.Tensor.atan_",
        "torch.Tensor.atanh",
        "torch.Tensor.atanh_",
        "torch.Tensor.bincount",
        "torch.Tensor.bitwise_not",
        "torch.Tensor.bitwise_not_",
        "torch.Tensor.ceil",
        "torch.Tensor.ceil_",
        "torch.Tensor.cholesky",
        "torch.Tensor.cholesky_inverse",
        "torch.Tensor.clip",
        "torch.Tensor.clip_",
        "torch.Tensor.coalesce",
        "torch.Tensor.conj",
        "torch.Tensor.cos_",
        "torch.Tensor.cosh",
        "torch.Tensor.cosh_",
        "torch.Tensor.cumprod",
        "torch.Tensor.cumprod_",
        "torch.Tensor.data_ptr",
        "torch.Tensor.deg2rad",
        "torch.Tensor.dense_dim",
        "torch.Tensor.detach_",
        "torch.Tensor.diag_embed",
        "torch.Tensor.diagflat",
        "torch.Tensor.digamma",
        "torch.Tensor.digamma_",
        "torch.Tensor.dtype",
        "torch.Tensor.erf",
        "torch.Tensor.erfinv",
        "torch.Tensor.erfinv_",
        "torch.Tensor.exp",
        "torch.Tensor.exp_",
        "torch.Tensor.expm1",
        "torch.Tensor.floor",
        "torch.Tensor.floor_",
        "torch.Tensor.frac",
        "torch.Tensor.frac_",
        "torch.Tensor.frexp",
        "torch.Tensor.grad",
        "torch.Tensor.i0",
        "torch.Tensor.i0_",
        "torch.Tensor.indices",
        "torch.Tensor.inverse",
        "torch.Tensor.is_complex",
        "torch.Tensor.is_floating_point",
        "torch.Tensor.is_leaf",
        "torch.Tensor.isfinite",
        "torch.Tensor.isinf",
        "torch.Tensor.isneginf",
        "torch.Tensor.isposinf",
        "torch.Tensor.isreal",
        "torch.Tensor.istft",
        "torch.Tensor.lgamma",
        "torch.Tensor.lgamma_",
        "torch.Tensor.log10",
        "torch.Tensor.log10_",
        "torch.Tensor.log1p",
        "torch.Tensor.log1p_",
        "torch.Tensor.log2",
        "torch.Tensor.log2_",
        "torch.Tensor.log_",
        "torch.Tensor.logit",
        "torch.Tensor.logit_",
        "torch.Tensor.lu",
        "torch.Tensor.mT",
        "torch.Tensor.masked_scatter_",
        "torch.Tensor.masked_select",
        "torch.Tensor.matrix_power",
        "torch.Tensor.mm",
        "torch.Tensor.moveaxis",
        "torch.Tensor.mv",
        "torch.Tensor.nan_to_num",
        "torch.Tensor.nan_to_num_",
        "torch.Tensor.ndimension",
        "torch.Tensor.neg",
        "torch.Tensor.neg_",
        "torch.Tensor.pin_memory",
        "torch.Tensor.polygamma",
        "torch.Tensor.polygamma_",
        "torch.Tensor.rad2deg",
        "torch.Tensor.reciprocal",
        "torch.Tensor.reciprocal_",
        "torch.Tensor.register_hook",
        "torch.Tensor.rsqrt",
        "torch.Tensor.rsqrt_",
        "torch.Tensor.sgn",
        "torch.Tensor.sigmoid_",
        "torch.Tensor.sign",
        "torch.Tensor.signbit",
        "torch.Tensor.sin_",
        "torch.Tensor.sinc",
        "torch.Tensor.sinc_",
        "torch.Tensor.sinh",
        "torch.Tensor.sinh_",
        "torch.Tensor.sparse_dim",
        "torch.Tensor.sqrt",
        "torch.Tensor.sqrt_",
        "torch.Tensor.t",
        "torch.Tensor.t_",
        "torch.Tensor.tan",
        "torch.Tensor.tan_",
        "torch.Tensor.tanh",
        "torch.Tensor.tanh_",
        "torch.Tensor.to_dense",
        "torch.Tensor.tril_",
        "torch.Tensor.triu_",
        "torch.Tensor.trunc",
        "torch.Tensor.trunc_",
        "torch.Tensor.values",
        "torch.__version__",
        "torch.__version__.split",
        "torch.diag_embed",
        "torch.distributed.ReduceOp.MAX",
        "torch.distributed.ReduceOp.MIN",
        "torch.distributed.ReduceOp.SUM",
        "torch.distributed.ReduceOp",
        "torch.distributed.batch_isend_irecv",
        "torch.distributed.get_backend",
        "torch.distributed.is_available",
        "torch.distributed.is_initialized",
        "torch.e",
        "torch.enable_grad",
        "torch.inf",
        "torch.is_grad_enabled",
        "torch.nan",
        "torch.newaxis",
        "torch.nn.LogSigmoid",
        "torch.nn.Sigmoid",
        "torch.nn.Softplus",
        "torch.nn.Softsign",
        "torch.nn.Tanh",
        "torch.nn.Tanhshrink",
        "torch.nn.TransformerDecoder",
        "torch.nn.TripletMarginWithDistanceLoss",
        "torch.nn.utils.parameters_to_vector",
        "torch.nn.utils.vector_to_parameters",
        "torch.pi",
        "torch.set_default_dtype",
        "torch.t",
        "torch.utils.cpp_extension.BuildExtension",
        "torch.utils.cpp_extension.BuildExtension.with_options",
        "torch.is_grad_enabled",
        "torch.nn.Conv2d",
        "torch.nn.init.calculate_gain",
        "torch.nn.init.ones_",
        "torch.nn.init.uniform_",
        "torch.nn.init.zeros_",
        "torch.Tensor.div",
        "torch.Tensor.element_size",
        "torch.Tensor.is_floating_point",
        "torch.Tensor.neg",
        "torch.Tensor.pin_memory",
        "torch.Tensor.view_as",
        "torch.distributed.is_available",
        "torch.distributed.is_initialized",
        "torch.set_default_dtype",
        "torch.dtype",
        "torch.Tensor.data_ptr",
        "torch.Tensor.__and__",
        "torch.Tensor.__array__",
        "torch.Tensor.__bool__",
        "torch.Tensor.__eq__",
        "torch.Tensor.__format__",
        "torch.Tensor.__getitem__",
        "torch.Tensor.__index__",
        "torch.Tensor.__invert__",
        "torch.Tensor.__len__",
        "torch.Tensor.__and__",
        "torch.Tensor.__or__",
        "torch.Tensor.__xor__",
        "torch.Tensor.__not__",
        "torch.Tensor.__ior__",
        "torch.Tensor.__rpow__",
        "torch.Tensor.__rsub__",
        "torch.Tensor.__rtruediv__",
        "torch.Tensor.__setitem__",
        
        # sundong
        "torch.matmul",
        "torch.linalg.matmul",
        "torch.mul",
        "torch.multiply",
        "torch.Tensor.mul",
        "torch.Tensor.mul_",
        "torch.Tensor.multiply",
        "torch.Tensor.multiply_",
        "torch.Tensor.matmul",
        "torch.amax",
        "torch.amin",
        "torch.Tensor.amax",
        "torch.Tensor.amin",
        "torch.Tensor.log2",
        "torch.log2",
        "torch.Tensor.remainder",
        "torch.remainder",
        
        # zhengsheng
        "torch.broadcast_to",
        "torch.nn.functional.embedding",
        "torch.no_grad",
        "torch.ones_like",
        "torch.reshape",
        "torch.take_along_dim",
        "torch.Tensor.bitwise_or_",
        "torch.Tensor.view",
        "torch.unique_consecutive",
        "torch.eye",
        "torch.full_like",
        "torch.Tensor.cumsum",
        "torch.Tensor.expand",
        "torch.clip",
        "torch.isfinite",
        "torch.isinf",
        "torch.isnan",
        "torch.flatten",
        "torch.Tensor.flatten",
        "torch.roll",
        "torch.Tensor.roll",
        "torch.Tensor.sum",
        "torch.sum",
        "torch.repeat_interleave",
        "torch.Tensor.repeat_interleave",
        "torch.var",
        "torch.prod",
        "torch.ceil",
        "torch.floor_divide",
        "torch.masked_select",
        "torch.index_put",
        "torch.Tensor.index_put",
        "torch.Tensor.index_put_",
        "torch.bucketize",
        "torch.searchsorted",
        
        # liuyi
        "torch.finfo",
        "torch.is_complex",
        "torch.concat",
        "torch.empty_like",
        "torch.full",
        "torch.nonzero",
        "torch.Tensor.pow",
        "torch.Tensor.prod",
        "torch.Tensor.reshape",
        "torch.zeros_like",
        "torch.argsort",
        "torch.Tensor.argsort",
        "torch.Tensor.squeeze",
        "torch.chunk",
        "torch.Tensor.chunk",
        "torch.any",
        "torch.nn.functional.one_hot",
        "torch.unbind",
        "torch.Tensor.unbind",
        "torch.is_floating_point",
        "torch.is_tensor",
        "torch.isin",
        
        # shenwei
        "torch.Tensor.expand_as",
        "torch.logsumexp",
        "torch.Tensor.logsumexp",
        "torch.argmax",
        "torch.Tensor.argmax",
        "torch.argmin",
        "torch.Tensor.argmin",
        "torch.all",
        "torch.Tensor.all",
        "torch.Tensor.any",
        "torch.tensor_split",
        "torch.nn.functional.gelu",
        "torch.nn.functional.grid_sample",
        
        # haoyang
        "torch.logical_not",
        "torch.Tensor.logical_not",
        "torch.logical_and",
        "torch.Tensor.logical_and",
        "torch.logical_or",
        "torch.Tensor.logical_or",
        "torch.logical_xor",
        "torch.Tensor.logical_xor",
        "torch.index_select",
        "torch.Tensor.index_select",
        "torch.dot",
        "torch.Tensor.dot",
        "torch.nn.functional.conv1d",
        "torch.nn.functional.conv2d",
        "torch.nn.functional.conv3d",
        "torch.conv1d",
        "torch.conv2d",
        "torch.conv3d",
        
        # zhichao
        "torch.bfloat16",
        "torch.bool",
        "torch.complex128",
        "torch.complex64",
        "torch.float64",
        "torch.float16",
        "torch.float32",
        "torch.int16",
        "torch.int32",
        "torch.int64",
        "torch.int8",
        "torch.ravel",
        "torch.Tensor.narrow",
        "torch.narrow",
        "torch.Tensor.type_as",
        "torch.nn.Sequential",
        
        # zhouwei
        "torch.float8_e4m3fn",
        "torch.abs",
        "torch.squeeze",
        "torch.randn_like",
        "torch.Tensor.__floordiv__",
        "torch.Tensor.__int__",
        #"torch.Tensor.is_cpu",
        #"torch.bitwise_or",
        #"torch.Tensor.bitwise_or",
        ##"torch.autograd.function.FunctionCtx.mark_dirty",
        #"torch.autograd.function.FunctionCtx.mark_non_differentiable",
        ##"torch.autograd.function.FunctionCtx.mark_shared_storage",
        #"torch.autograd.function.FunctionCtx.save_for_backward",
        ##"torch.autograd.function.FunctionCtx.save_for_forward",
        #"torch.autograd.function.FunctionCtx.set_materialize_grads",
        ##"torch.autograd.function.FunctionCtx.saved_tensors",
        #"torch.autograd.function.once_differentiable",
        "torch.nn.PixelUnshuffle",
        "torch.nn.Upsample",
        "torch.nn.PixelShuffle",
        "torch.nn.LocalResponseNorm",
        "torch.nn.ChannelShuffle",
        "torch.nn.AdaptiveLogSoftmaxWithLoss",
        "torch.Tensor.corrcoef",
        "torch.Tensor.multinomial",
        "torch.get_rng_state",
        "torch.Tensor.trace",
        "torch.Tensor.clone",
        "torch.Tensor.contiguous",
        "torch.Tensor.is_contiguous",
        "torch.utils.cpp_extension.CUDA_HOME",
        "torch.Tensor.is_coalesced",
        "torch.torch.int32",
        "torch.transpose",
        "torch.Tensor.transpose",
        "torch.unsqueeze",
        "torch.Tensor.unsqueeze",
        "torch.sigmoid",
        "torch.Tensor.topk",
        "torch.outer",
        "torch.nn.functional.sigmoid",
        "torch.Tensor.requires_grad",
        "torch.Tensor.data",
        "torch.Tensor.element_size",
        "torch.Tensor.view_as",
        "torch.Tensor.cpu",
        "torch.Tensor.__add__",
        "torch.Tensor.__ge__",
        "torch.Tensor.__gt__",
        "torch.Tensor.__le__",
        "torch.Tensor.__lt__",
        "torch.Tensor.__mul__",
        "torch.Tensor.__ne__",
        "torch.Tensor.__neg__",
        "torch.Tensor.__radd__",
        "torch.Tensor.__rmul__",
        "torch.Tensor.__sub__",
        "torch.Tensor.__reduce_ex__",
        "torch.from_numpy",
        "torch.greater",
        "torch.nn.functional.layer_norm",
        "torch.nn.attention.sdpa_kernel",
        "torch.nn.attention.SDPBackend",
        "torch.nn.attention.SDPBackend.ERROR",
        "torch.nn.attention.SDPBackend.MATH",
        "torch.nn.attention.SDPBackend.FLASH_ATTENTION",
        "torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION",
        "torch.nn.attention._cur_sdpa_kernel_backends",
        "torch.Tensor.remainder",
        "torch.nn.SiLU",
        "torch.randperm",
        "torch.Tensor.ge",
        "torch.Tensor.gt",
        "torch.Tensor.le",
        "torch.Tensor.lt",
        "torch.Tensor.ne",
        "torch.manual_seed",
        "torch.asarray",
        "torch.newaxis",
        "torch.inf",
        "torch.nan",
        "torch.pi",
        "torch.e",
        "torch.cat",
        "torch.concatenate",
        "torch.take_along_dim",
        "torch.clamp",
        "torch.ger",
        "torch.div",
        "torch.Tensor.div_",
        "torch.eq",
        "torch.not_equal",
        "torch.ne",
        "torch.less",
        "torch.lt",
        "torch.less_equal",
        "torch.le",
        "torch.greater",
        "torch.gt",
        "torch.greater_equal",
        "torch.ge",
        "torch.Tensor.eq",
        "torch.Tensor.not_equal",
        "torch.Tensor.ne",
        "torch.Tensor.less",
        "torch.Tensor.lt",
        "torch.Tensor.less_equal",
        "torch.Tensor.le",
        "torch.Tensor.greater",
        "torch.Tensor.gt",
        "torch.Tensor.greater_equal",
        "torch.Tensor.ge",
        # "torch.Tensor.eq_",
        # "torch.Tensor.not_equal_",
        # "torch.Tensor.ne_",
        # "torch.Tensor.less_",
        # "torch.Tensor.lt_",
        # "torch.Tensor.less_equal_",
        # "torch.Tensor.le_",
        # "torch.Tensor.greater_",
        # "torch.Tensor.gt_",
        # "torch.Tensor.greater_equal_",
        # "torch.Tensor.ge_",
        "torch.swapdims",
        "torch.swapaxes",
        "torch.manual_seed",
        "torch.sub",
        "torch.Tensor.sub_",
        "torch.get_default_device",
        "torch.Tensor.eq",
        "torch.Tensor.ne",
        "torch.Tensor.lt",
        "torch.Tensor.less",
        "torch.Tensor.le",
        "torch.Tensor.greater",
        "torch.Tensor.ge",
        "torch.Tensor.swapdims",
        "torch.Tensor.swapaxes",
        "torch.Tensor.sub",
        "torch.isclose",
        "torch.special.expm1",
        "torch.Tensor.requires_grad_",
        "torch.Tensor.__deepcopy__",
        "torch.Tensor.__pow__",
        "torch.hamming_window",
        "torch.blackman_window",
        "torch.nn.functional.softplus",
        "torch.Tensor.index_add_",
        "torch.fft.rfft",
        "torch.fft.fft",

        # qianyue
        "torch.gather",
        "torch.Tensor.gather",
        "torch.Tensor.scatter",
        "torch.Tensor.scatter_",
        "torch.scatter",

        #lijinjin 
        "torch.Tensor.clamp_",
        "torch.nn.functional.logsigmoid",
        "torch.autocast",
        "torch.nn.init._calculate_fan_in_and_fan_out",
        "torch.functional.meshgrid",

        # xiangyu
        "torch.cuda.get_device_properties",
        "torch.cuda.get_rng_state",
        "torch.cuda.is_current_stream_capturing",
        "torch.cuda.manual_seed_all",
        "torch.cuda.set_rng_state",
        "torch.get_default_device",
        "torch.get_device_module",
        "torch.cuda.is_available",
        "torch.device",
        "torch.cuda.get_device_capability",
        "torch.cuda.device",
        "torch.cuda.manual_seed",
        "torch.cuda.max_memory_allocated",
        'torch.cuda.max_memory_reserved',
        "torch.cuda.reset_peak_memory_stats",
        "torch.cuda.set_stream",
        "torch.cuda.Event",
        "torch.get_device",
        "torch.cuda.FloatTensor",
        "torch.cuda.DoubleTensor",
        "torch.cuda.HalfTensor",
        "torch.cuda.BFloat16Tensor",
        "torch.cuda.ByteTensor",
        "torch.cuda.CharTensor",
        "torch.cuda.ShortTensor",
        "torch.cuda.IntTensor",
        "torch.cuda.LongTensor",
        "torch.cuda.BoolTensor",
        "torch.cuda.nvtx.range_push",
        "torch.cuda.nvtx.range_pop",
        "torch.cuda.amp.autocast",
        "torch.cuda.amp.autocast_mode.autocast",
        "torch.amp.autocast",
        "torch.cuda.get_device_name",
        "torch.cuda.stream",
        "torch.cuda.StreamContext",
        "torch.Generator",
        #"torch.cuda.CUDAGraph",
        #"torch.cuda.graph_pool_handle",
        "torch.cuda.ipc_collect",
        "torch.cuda.synchronize",
        "torch.cuda.Stream",
        "torch.cuda.is_bf16_supported",
        "torch.Tensor.device",


        # geyuqiang
        "torch.cuda.current_device",
        "torch.cuda.device_count",
        "torch.cuda.empty_cache",
        "torch.cuda.memory_allocated",
        "torch.cuda.memory_reserved",
        "torch.cuda.set_device",
        "torch.cuda.current_stream",

        # genghaozhe
        "torch.nn.functional.interpolate",
        
        # ooooo
        "torch.nn.AdaptiveAvgPool1d",
        "torch.nn.AdaptiveAvgPool2d",
        "torch.nn.AdaptiveAvgPool3d",
        "torch.nn.HuberLoss",
        "torch.nn.MaxUnpool1d",
        "torch.nn.MaxUnpool2d",
        "torch.nn.MaxUnpool3d",
        "torch.nn.UpsamplingBilinear2d",
        "torch.nn.UpsamplingNearest2d",
        "torch.nn.ZeroPad1d",
        "torch.nn.ZeroPad2d",
        "torch.nn.ZeroPad3d",
        "torch.adaptive_avg_pool1d",
        "torch.nn.functional.adaptive_avg_pool1d",
        "torch.nn.functional.adaptive_avg_pool2d",
        "torch.nn.functional.adaptive_avg_pool3d",

        "torch.nn.GLU",
        "torch.nn.Hardshrink",
        "torch.nn.Softshrink",
        "torch.nn.CosineSimilarity",
        "torch.nn.Fold",
        "torch.nn.PairwiseDistance",
        "torch.nn.LPPool1d",
        "torch.nn.LPPool2d",
        "torch.nn.AdaptiveMaxPool1d",
        "torch.nn.AdaptiveMaxPool2d",
        "torch.nn.AdaptiveMaxPool3d",

        "torch.nn.functional.hardshrink",
        "torch.nn.functional.softshrink",
        "torch.nn.functional.glu",
        "torch.nn.functional.cosine_similarity",
        "torch.nn.functional.fold",
        "torch.nn.functional.pairwise_distance",
        "torch.nn.functional.adaptive_max_pool1d",
        "torch.nn.functional.adaptive_max_pool2d",
        "torch.nn.functional.adaptive_max_pool3d",
        "torch.nn.functional.lp_pool1d",
        "torch.nn.functional.lp_pool2d",

        # algorithm1832
        "torch.cosh",
        "torch.frac",
        "torch.diag",
        "torch.Tensor.diag",
        
        # lijialin
        "torch.group_norm",
        "torch.layer_norm",
        "torch.nn.functional.group_norm",
        "torch.nn.GroupNorm",
        "torch.nn.LayerNorm",

        #enkilee
        "torch.acos",
        "torch.acosh",
        "torch.atanh",
        "torch.sinh",

        #fangfangssj
        "torch.abs",
        "torch.Tensor.abs",
        "torch.cumprod",
        "torch.Tensor.cumprod",
        "torch.exp",
        "torch.Tensor.exp",
        "torch.expm1",
        "torch.Tensor.expm1",
        "torch.diagonal",
        "torch.Tensor.diagonal",
        "torch.round",
        "torch.Tensor.round",
        "torch.tanh",
        "torch.Tensor.tanh",
        "torch.nn.functional.tanh",
        "torch.index_add",
        "torch.Tensor.index_add",
        "torch.Tensor.index_add_",
        "torch.linalg.solve",
        "torch.nn.functional.normalize",
        "torch.quantile",
        "torch.Tensor.quantile",
        "torch.unflatten",
        "torch.Tensor.unflatten",
        "torch.Tensor.uniform_",
        "torch.Tensor.unfold",
        "torch.testing.assert_close",
        "torch.fft.fft",
        "torch.fft.ifft",
        "torch.fft.fft2",
        "torch.fft.ifft2",
        "torch.fft.fftn",
        "torch.fft.ifftn",
        "torch.fft.rfft",
        "torch.fft.irfft",
        "torch.fft.rfft2",
        "torch.fft.irfft2",
        "torch.fft.rfftn",
        "torch.fft.irfftn",
        "torch.fft.hfft",
        "torch.fft.ihfft",
        "torch.fft.hfft2",
        "torch.fft.ihfft2",
        "torch.fft.hfftn",
        "torch.fft.ihfftn",
        "torch.fft.fftshift",
        "torch.fft.ifftshift",

        "torch.tile",
    ]

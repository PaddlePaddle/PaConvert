import paddle
import paddlenlp


class A(paddle.nn.Layer):
    def __init__(self, data: paddle.Tensor):
        pass


def func1(data: paddle.Tensor):
>>>>>>    torch.utils.data(x)
>>>>>>    torch.utils.data.data_loader(x)
    data["id"]
    func(data)
    data = json.load(f)
    self.data = self.__make_dataset()


def func2(data) -> paddle.Tensor:
    pass


def func3(dtype=paddle.float32):
    pass


isinstance(x, paddle.Tensor)
setattr(paddle.Tensor, "add", add_func)
hasattr(paddle.Tensor, "add")
Union[paddlenlp.transformers.model_outputs.BaseModelOutput, paddle.LongTensor]
Optional[paddle.Tensor] = None
my_add = paddle.add
setattr(paddle.nn, "functional", my_functional_module)
>>>>>>transformers.activations.ACT2FN["tanh"]
>>>>>>transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS["flash_attention_2"]
>>>>>>transformers.modeling_rope_utils.ROPE_INIT_FUNCTIONS["linear"]

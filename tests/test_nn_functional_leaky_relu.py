# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import textwrap

from apibase import APIBase

obj = APIBase("torch.nn.functional.leaky_relu")


def test_case_1():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(x)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_2():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(x, 0.06)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_3():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(input=x, negative_slope=0.08)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_4():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(x, 0.09, inplace=True)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_5():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(input=x, inplace=False)
        """
    )
    obj.run(pytorch_code, ["result"])


# generated by validate_unittest autofix, based on test_case_4
def test_case_6():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(x, 0.09, True)
        """
    )
    obj.run(pytorch_code, ["result"])


# generated by validate_unittest autofix, based on test_case_4
def test_case_7():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(input=x, negative_slope=0.09, inplace=True)
        """
    )
    obj.run(pytorch_code, ["result"])


# generated by validate_unittest autofix, based on test_case_4
def test_case_8():
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.3020, -0.1005,  0.5766,  0.6351, -0.8893,  0.0253, -0.1756, 1.2913],
                            [-0.8833, -0.1369, -0.0168, -0.5409, -0.1511, -0.1240, -1.1870, -1.8816]]])
        result = F.leaky_relu(inplace=True, negative_slope=0.09, input=x)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_9():
    """1D tensor input"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-1.5, -0.5, 0.0, 0.5, 1.5])
        result = F.leaky_relu(x)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_10():
    """2D tensor input"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[-1.0, 2.0], [3.0, -4.0]])
        result = F.leaky_relu(x, 0.1)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_11():
    """float64 dtype"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-1.5, -0.5, 0.0, 0.5, 1.5], dtype=torch.float64)
        result = F.leaky_relu(x, negative_slope=0.2)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_12():
    """4D tensor (batch, channel, height, width)"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[[-1.0, 2.0], [3.0, -4.0]], [[0.5, -0.5], [-1.5, 1.5]]]])
        result = F.leaky_relu(x, 0.1)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_13():
    """All negative values"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-1.0, -2.0, -3.0, -4.0])
        result = F.leaky_relu(x, 0.2)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_14():
    """Default negative_slope (0.01)"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-10.0, -5.0, 0.0, 5.0, 10.0])
        result = F.leaky_relu(x)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_15():
    """Gradient computation"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True)
        result = F.leaky_relu(x, 0.1)
        result.sum().backward()
        x_grad = x.grad
        """
    )
    obj.run(pytorch_code, ["result", "x_grad"], check_stop_gradient=False)


def test_case_16():
    """Large negative_slope"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
        result = F.leaky_relu(x, negative_slope=0.5)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_17():
    """Expression as input"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([1.0, 2.0, 3.0])
        result = F.leaky_relu(x - 2.0, 0.1)
        """
    )
    obj.run(pytorch_code, ["result"])


def test_case_18():
    """3D tensor"""
    pytorch_code = textwrap.dedent(
        """
        import torch
        import torch.nn.functional as F
        x = torch.tensor([[[-1.0, 0.5, 1.0], [2.0, -0.5, -1.5]]])
        result = F.leaky_relu(x, negative_slope=0.15)
        """
    )
    obj.run(pytorch_code, ["result"])
